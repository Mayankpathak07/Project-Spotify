{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bbe056-3738-41d1-b1cd-840d69bed467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test=dbutils.secrets.get(scope=\"my-secret\",key=\"client-id\")\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be1d65a-cb53-4c8e-9ac3-9aa760bde297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount Azure Data Lake Gen2 container using OAuth\n",
    "\n",
    "# Define the storage account and container names\n",
    "storage_account_name = \"dlprojectspotify\"\n",
    "container_name = \"bronze\"\n",
    "mount_point = \"/mnt/bronze\"\n",
    "\n",
    "# Define the Key Vault secret scope and secret name\n",
    "secret_scope = \"my-secret\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope, key=\"ClientId\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope, key=\"tenantid\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope, key=\"secretvalue\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22518db-28aa-4be2-959f-ba67b8a64da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/bronze\"\n",
    "\n",
    "# Check if already mounted\n",
    "mounts = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "if mount_point not in mounts:\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://bronze@dlprojectspotify.dfs.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=configs\n",
    "    )\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681356a6-7b6e-4b9b-81fe-279d6117df84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "        \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/\"+tenant_id+\"/oauth2/token\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Mount the container\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://bronze@dlprojectspotify.dfs.core.windows.net/\",\n",
    "  mount_point = mount_point,\n",
    "  extra_configs = configs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606622dd-fee1-46c5-9740-3130bd502170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a=dbutils.fs.ls(\"/mnt/bronze/Bronze\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a685b376-13a3-4e99-9ab4-cbc5e333e511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df=spark.read.option(\"header\",True).csv(\"/mnt/bronze/Bronze/universal_top_spotify_songs.csv\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec00bde-14b9-444f-a8d7-872210456499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7556acb9-2258-4fe4-85de-434fa9317911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76efe400-abe2-4110-8bf6-700ee1502e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dc1648-2a92-4400-9ffd-191d7fd625b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "null_counts = {}\n",
    "\n",
    "# Iterate through columns\n",
    "for column_name in df.columns:\n",
    "    # Count the number of nulls or NaNs for each column and store it in the dictionary\n",
    "    null_counts[column_name] = df.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "\n",
    "# Display the results\n",
    "for col_name, count_value in null_counts.items():\n",
    "    print(f\"{col_name}: {count_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6621a92-1b7c-4fb6-a9cb-77a3dbcb004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4d1e5d-571d-4ad8-97e6-5a6f22b98421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c42523f-f2ba-42e0-ad8b-e71e806cbbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"key\", \"mode\", \"time_signature\", \"daily_movement\", \"weekly_movement\"]\n",
    "df_cleaned = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1584a5-30b0-46dd-881a-8f500f9df375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform=df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2870c55d-3626-4a86-8515-8a85f93105fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "null_counts = {}\n",
    "\n",
    "# Iterate through columns\n",
    "for column_name in dftransform.columns:\n",
    "    # Count the number of nulls or NaNs for each column and store it in the dictionary\n",
    "    null_counts[column_name] = dftransform.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "\n",
    "# Display the results\n",
    "for col_name, count_value in null_counts.items():\n",
    "    print(f\"{col_name}: {count_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0d91ab-94de-4e36-a35b-7bf2a821e51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f230f5a6-c6d2-4465-b2f7-75d69aeb4457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dup=dftransform.distinct()\n",
    "df_dup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2796c8-cfac-4fd0-9ca2-7735cde30663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "dftransform=dftransform.withColumn(\"daily_rank\",col(\"daily_rank\").cast(\"int\"))\n",
    "dftransform.schema[\"daily_rank\"].dataType\n",
    "dftransform=dftransform.withColumn(\"snapshot_date\",to_date(\"snapshot_date\",\"yyyy-MM-dd\"))\n",
    "dftransform.schema[\"snapshot_date\"].dataType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989f0280-0085-43c6-bb10-7ca84be51bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dftransform = dftransform.withColumn(\"popularity\", col(\"popularity\").cast(\"int\")) \\\n",
    "       .withColumn(\"duration_ms\", col(\"duration_ms\").cast(\"double\")) \\\n",
    "       .withColumn(\"album_release_date\", to_date(\"album_release_date\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"danceability\", col(\"danceability\").cast(\"double\")) \\\n",
    "       .withColumn(\"energy\", col(\"energy\").cast(\"double\")) \\\n",
    "       .withColumn(\"loudness\", col(\"loudness\").cast(\"double\")) \\\n",
    "       .withColumn(\"speechiness\", col(\"speechiness\").cast(\"double\")) \\\n",
    "       .withColumn(\"acousticness\", col(\"acousticness\").cast(\"double\")) \\\n",
    "       .withColumn(\"instrumentalness\", col(\"instrumentalness\").cast(\"double\")) \\\n",
    "       .withColumn(\"liveness\", col(\"liveness\").cast(\"double\")) \\\n",
    "       .withColumn(\"valence\", col(\"valence\").cast(\"double\")) \\\n",
    "       .withColumn(\"tempo\", col(\"tempo\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1c9e8f-1b53-4420-93cd-71867d80e703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d4b1d4-5f40-447e-81f0-38a0fec8dcff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.withColumn(\"duration_minutes\", (col(\"duration_ms\") / 60000).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8075406-2edb-481d-b74e-bc7544074bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "string_columns = [\"spotify_id\", \"name\", \"artists\", \"country\", \"album_name\"]\n",
    "\n",
    "for col_name in string_columns:\n",
    "    dftransform = dftransform.withColumn(col_name, trim(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70aa6a47-4bb4-4ad5-8517-39ccdde47141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "dftransform = dftransform.withColumn(\"is_explicit\", when(col(\"is_explicit\") == \"1\", \"Yes\")\n",
    "                                       .when(col(\"is_explicit\") == \"0\", \"No\")\n",
    "                                       .otherwise(col(\"is_explicit\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e592085-fe8a-4912-9c83-5ca8e42e30b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.filter(col(\"popularity\") >= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32e40f1-e595-4b0d-9915-fbbddb355f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219566c5-8b5e-440b-a19a-06b545b06296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe([\"danceability\", \"energy\", \"loudness\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0f49bc-bc11-4d14-aa88-925e53911027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ordered_columns = [\n",
    "    \"spotify_id\", \"name\", \"artists\", \"album_name\",\n",
    "    \"album_release_date\", \"snapshot_date\",\n",
    "    \"country\", \"daily_rank\", \"popularity\", \"is_explicit\", \n",
    "    \"duration_ms\", \"duration_minutes\",\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\",\n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c483b86f-e073-4328-87b9-5111a78477ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Reorder columns\n",
    "dftransform = dftransform.select(*[\n",
    "    \"spotify_id\", \"name\", \"artists\", \"album_name\",\n",
    "    \"album_release_date\", \"snapshot_date\",\n",
    "    \"country\", \"daily_rank\", \"popularity\", \"is_explicit\",\n",
    "    \"duration_ms\", \"duration_minutes\",\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\",\n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "])\n",
    "\n",
    "# 2. Validate columns\n",
    "\n",
    "# List of columns that should be between 0 and 1\n",
    "columns_0_1 = [\"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\"]\n",
    "\n",
    "# List of problems found\n",
    "problems = []\n",
    "\n",
    "# Validate 0-1 range columns\n",
    "for col_name in columns_0_1:\n",
    "    invalid = dftransform.filter((col(col_name) < 0) | (col(col_name) > 1)).count()\n",
    "    if invalid > 0:\n",
    "        problems.append(f\"{col_name} has {invalid} invalid rows outside [0,1]\")\n",
    "\n",
    "# Validate loudness (usually between -60 and 0)\n",
    "invalid_loudness = dftransform.filter((col(\"loudness\") < -60) | (col(\"loudness\") > 0)).count()\n",
    "if invalid_loudness > 0:\n",
    "    problems.append(f\"loudness has {invalid_loudness} values outside [-60, 0] range\")\n",
    "\n",
    "# Validate tempo (positive, usually 20-300 bpm)\n",
    "invalid_tempo = dftransform.filter((col(\"tempo\") <= 0) | (col(\"tempo\") > 300)).count()\n",
    "if invalid_tempo > 0:\n",
    "    problems.append(f\"tempo has {invalid_tempo} invalid values (should be >0 and reasonable)\")\n",
    "\n",
    "# Validate duration_ms and duration_minutes (should be positive)\n",
    "for col_name in [\"duration_ms\", \"duration_minutes\"]:\n",
    "    invalid = dftransform.filter(col(col_name) <= 0).count()\n",
    "    if invalid > 0:\n",
    "        problems.append(f\"{col_name} has {invalid} non-positive values\")\n",
    "\n",
    "# Validate daily_rank (should be positive integers)\n",
    "invalid_daily_rank = dftransform.filter(col(\"daily_rank\") <= 0).count()\n",
    "if invalid_daily_rank > 0:\n",
    "    problems.append(f\"daily_rank has {invalid_daily_rank} non-positive values\")\n",
    "\n",
    "# Validate popularity (should be between 0 and 100)\n",
    "invalid_popularity = dftransform.filter((col(\"popularity\") < 0) | (col(\"popularity\") > 100)).count()\n",
    "if invalid_popularity > 0:\n",
    "    problems.append(f\"popularity has {invalid_popularity} invalid values (should be 0-100)\")\n",
    "\n",
    "# Show results\n",
    "if problems:\n",
    "    print(\"Found issues:\")\n",
    "    for p in problems:\n",
    "        print(\"-\", p)\n",
    "else:\n",
    "    print(\"✅ All validations passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2269028-64fa-4016-b9f3-7bddaaa7b5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Drop rows where 0-1 columns are invalid\n",
    "columns_0_1 = [\"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\"]\n",
    "\n",
    "condition_0_1 = \" AND \".join([f\"({col_name} >= 0 AND {col_name} <= 1)\" for col_name in columns_0_1])\n",
    "\n",
    "# 2. Drop rows where loudness is invalid\n",
    "condition_loudness = \"(loudness >= -60 AND loudness <= 0)\"\n",
    "\n",
    "# 3. Drop rows where tempo is invalid\n",
    "condition_tempo = \"(tempo > 0 AND tempo <= 300)\"\n",
    "\n",
    "# 4. Drop rows where duration is invalid\n",
    "condition_duration = \"(duration_ms > 0 AND duration_minutes > 0)\"\n",
    "\n",
    "# 5. Combine all conditions\n",
    "full_condition = f\"{condition_0_1} AND {condition_loudness} AND {condition_tempo} AND {condition_duration}\"\n",
    "\n",
    "# 6. Apply filter\n",
    "dftransform = dftransform.filter(full_condition)\n",
    "\n",
    "print(\"✅ Invalid rows dropped successfully.\")\n",
    "print(f\"🧮 New number of rows: {dftransform.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8970d6f-c84f-4eee-8172-cfd75438b1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Add release_year column\n",
    "dftransform = dftransform.withColumn(\"album_release_year\", year(col(\"album_release_date\")))\n",
    "\n",
    "print(\"✅ Created new column 'album_release_year'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425588cb-f60f-4acf-9428-0e7084dbd975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.withColumnRenamed('name', 'track_name') \\\n",
    "    .withColumnRenamed('artists', 'artist_name') \\\n",
    "    .withColumnRenamed('loudness', 'loudness_db') \\\n",
    "    .withColumnRenamed('tempo', 'tempo_bpm')\n",
    "# (Rest are already clean and good)\n",
    "\n",
    "print(\"✅ Columns renamed for better readability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f351e9df-e216-4f47-92a5-9f1658bbb3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desired_order = [\n",
    "    'spotify_id', 'track_name', 'artist_name', 'daily_rank', 'country', 'snapshot_date',\n",
    "    'popularity', 'is_explicit', 'duration_ms', 'duration_minutes',\n",
    "    'album_name', 'album_release_date', 'album_release_year',\n",
    "    'danceability', 'energy', 'loudness_db', 'speechiness', 'acousticness',\n",
    "    'instrumentalness', 'liveness', 'valence', 'tempo_bpm'\n",
    "]\n",
    "dftransform = dftransform.select(*desired_order)\n",
    "\n",
    "print(\"✅ Columns rearranged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617a4ee8-add8-471d-bb79-00fba49dcc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277e5de2-f8ea-4061-ac83-1931d561eaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/bronze/Silver/spotify_tracks\"\n",
    "dftransform.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n",
    "print(\"✅ Data successfully saved to Silver layer in Delta format!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46dda7d-adea-4f94-8ba9-2c69d354bdd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsilver=spark.read.format(\"delta\").load(\"/mnt/bronze/Silver/spotify_tracks\")\n",
    "dfsilver.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e02408d-e9eb-4bac-9e9c-ccdbc370ddf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsilver.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d895592-7ee2-4ee7-802d-cfb046ff03c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "projects-spotify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
