{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be1d65a-cb53-4c8e-9ac3-9aa760bde297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount Azure Data Lake Gen2 container using OAuth\n",
    "\n",
    "# Define the storage account and container names\n",
    "storage_account_name = \"dlprojectspotify\"\n",
    "container_name = \"bronze\"\n",
    "mount_point = \"/mnt/bronze\"\n",
    "\n",
    "# Define the Key Vault secret scope and secret name\n",
    "secret_scope = \"my-secret\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope, key=\"ClientId\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope, key=\"tenantid\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope, key=\"secretvalue\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22518db-28aa-4be2-959f-ba67b8a64da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/bronze\"\n",
    "\n",
    "# Check if already mounted\n",
    "mounts = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "if mount_point not in mounts:\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://bronze@dlprojectspotify.dfs.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=configs\n",
    "    )\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681356a6-7b6e-4b9b-81fe-279d6117df84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "        \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/\"+tenant_id+\"/oauth2/token\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Mount the container\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://bronze@dlprojectspotify.dfs.core.windows.net/\",\n",
    "  mount_point = mount_point,\n",
    "  extra_configs = configs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606622dd-fee1-46c5-9740-3130bd502170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a=dbutils.fs.ls(\"/mnt/bronze/Bronze\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a685b376-13a3-4e99-9ab4-cbc5e333e511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df=spark.read.option(\"header\",True).csv(\"/mnt/bronze/Bronze/universal_top_spotify_songs.csv\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec00bde-14b9-444f-a8d7-872210456499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7556acb9-2258-4fe4-85de-434fa9317911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76efe400-abe2-4110-8bf6-700ee1502e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dc1648-2a92-4400-9ffd-191d7fd625b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "null_counts = {}\n",
    "\n",
    "# Iterate through columns\n",
    "for column_name in df.columns:\n",
    "    # Count the number of nulls or NaNs for each column and store it in the dictionary\n",
    "    null_counts[column_name] = df.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "\n",
    "# Display the results\n",
    "for col_name, count_value in null_counts.items():\n",
    "    print(f\"{col_name}: {count_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6621a92-1b7c-4fb6-a9cb-77a3dbcb004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4d1e5d-571d-4ad8-97e6-5a6f22b98421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c42523f-f2ba-42e0-ad8b-e71e806cbbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"key\", \"mode\", \"time_signature\", \"daily_movement\", \"weekly_movement\"]\n",
    "df_cleaned = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1584a5-30b0-46dd-881a-8f500f9df375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform=df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2870c55d-3626-4a86-8515-8a85f93105fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "null_counts = {}\n",
    "\n",
    "# Iterate through columns\n",
    "for column_name in dftransform.columns:\n",
    "    # Count the number of nulls or NaNs for each column and store it in the dictionary\n",
    "    null_counts[column_name] = dftransform.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "\n",
    "# Display the results\n",
    "for col_name, count_value in null_counts.items():\n",
    "    print(f\"{col_name}: {count_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0d91ab-94de-4e36-a35b-7bf2a821e51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f230f5a6-c6d2-4465-b2f7-75d69aeb4457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dup=dftransform.distinct()\n",
    "df_dup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2796c8-cfac-4fd0-9ca2-7735cde30663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "dftransform=dftransform.withColumn(\"daily_rank\",col(\"daily_rank\").cast(\"int\"))\n",
    "dftransform.schema[\"daily_rank\"].dataType\n",
    "dftransform=dftransform.withColumn(\"snapshot_date\",to_date(\"snapshot_date\",\"yyyy-MM-dd\"))\n",
    "dftransform.schema[\"snapshot_date\"].dataType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989f0280-0085-43c6-bb10-7ca84be51bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dftransform = dftransform.withColumn(\"popularity\", col(\"popularity\").cast(\"int\")) \\\n",
    "       .withColumn(\"duration_ms\", col(\"duration_ms\").cast(\"double\")) \\\n",
    "       .withColumn(\"album_release_date\", to_date(\"album_release_date\", \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"danceability\", col(\"danceability\").cast(\"double\")) \\\n",
    "       .withColumn(\"energy\", col(\"energy\").cast(\"double\")) \\\n",
    "       .withColumn(\"loudness\", col(\"loudness\").cast(\"double\")) \\\n",
    "       .withColumn(\"speechiness\", col(\"speechiness\").cast(\"double\")) \\\n",
    "       .withColumn(\"acousticness\", col(\"acousticness\").cast(\"double\")) \\\n",
    "       .withColumn(\"instrumentalness\", col(\"instrumentalness\").cast(\"double\")) \\\n",
    "       .withColumn(\"liveness\", col(\"liveness\").cast(\"double\")) \\\n",
    "       .withColumn(\"valence\", col(\"valence\").cast(\"double\")) \\\n",
    "       .withColumn(\"tempo\", col(\"tempo\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1c9e8f-1b53-4420-93cd-71867d80e703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d4b1d4-5f40-447e-81f0-38a0fec8dcff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.withColumn(\"duration_minutes\", (col(\"duration_ms\") / 60000).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8075406-2edb-481d-b74e-bc7544074bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "string_columns = [\"spotify_id\", \"name\", \"artists\", \"country\", \"album_name\"]\n",
    "\n",
    "for col_name in string_columns:\n",
    "    dftransform = dftransform.withColumn(col_name, trim(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70aa6a47-4bb4-4ad5-8517-39ccdde47141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "dftransform = dftransform.withColumn(\"is_explicit\", when(col(\"is_explicit\") == \"1\", \"Yes\")\n",
    "                                       .when(col(\"is_explicit\") == \"0\", \"No\")\n",
    "                                       .otherwise(col(\"is_explicit\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e592085-fe8a-4912-9c83-5ca8e42e30b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.filter(col(\"popularity\") >= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32e40f1-e595-4b0d-9915-fbbddb355f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219566c5-8b5e-440b-a19a-06b545b06296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe([\"danceability\", \"energy\", \"loudness\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0f49bc-bc11-4d14-aa88-925e53911027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ordered_columns = [\n",
    "    \"spotify_id\", \"name\", \"artists\", \"album_name\",\n",
    "    \"album_release_date\", \"snapshot_date\",\n",
    "    \"country\", \"daily_rank\", \"popularity\", \"is_explicit\", \n",
    "    \"duration_ms\", \"duration_minutes\",\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\",\n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c483b86f-e073-4328-87b9-5111a78477ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Reorder columns\n",
    "dftransform = dftransform.select(*[\n",
    "    \"spotify_id\", \"name\", \"artists\", \"album_name\",\n",
    "    \"album_release_date\", \"snapshot_date\",\n",
    "    \"country\", \"daily_rank\", \"popularity\", \"is_explicit\",\n",
    "    \"duration_ms\", \"duration_minutes\",\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\",\n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "])\n",
    "\n",
    "# 2. Validate columns\n",
    "\n",
    "# List of columns that should be between 0 and 1\n",
    "columns_0_1 = [\"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\"]\n",
    "\n",
    "# List of problems found\n",
    "problems = []\n",
    "\n",
    "# Validate 0-1 range columns\n",
    "for col_name in columns_0_1:\n",
    "    invalid = dftransform.filter((col(col_name) < 0) | (col(col_name) > 1)).count()\n",
    "    if invalid > 0:\n",
    "        problems.append(f\"{col_name} has {invalid} invalid rows outside [0,1]\")\n",
    "\n",
    "# Validate loudness (usually between -60 and 0)\n",
    "invalid_loudness = dftransform.filter((col(\"loudness\") < -60) | (col(\"loudness\") > 0)).count()\n",
    "if invalid_loudness > 0:\n",
    "    problems.append(f\"loudness has {invalid_loudness} values outside [-60, 0] range\")\n",
    "\n",
    "# Validate tempo (positive, usually 20-300 bpm)\n",
    "invalid_tempo = dftransform.filter((col(\"tempo\") <= 0) | (col(\"tempo\") > 300)).count()\n",
    "if invalid_tempo > 0:\n",
    "    problems.append(f\"tempo has {invalid_tempo} invalid values (should be >0 and reasonable)\")\n",
    "\n",
    "# Validate duration_ms and duration_minutes (should be positive)\n",
    "for col_name in [\"duration_ms\", \"duration_minutes\"]:\n",
    "    invalid = dftransform.filter(col(col_name) <= 0).count()\n",
    "    if invalid > 0:\n",
    "        problems.append(f\"{col_name} has {invalid} non-positive values\")\n",
    "\n",
    "# Validate daily_rank (should be positive integers)\n",
    "invalid_daily_rank = dftransform.filter(col(\"daily_rank\") <= 0).count()\n",
    "if invalid_daily_rank > 0:\n",
    "    problems.append(f\"daily_rank has {invalid_daily_rank} non-positive values\")\n",
    "\n",
    "# Validate popularity (should be between 0 and 100)\n",
    "invalid_popularity = dftransform.filter((col(\"popularity\") < 0) | (col(\"popularity\") > 100)).count()\n",
    "if invalid_popularity > 0:\n",
    "    problems.append(f\"popularity has {invalid_popularity} invalid values (should be 0-100)\")\n",
    "\n",
    "# Show results\n",
    "if problems:\n",
    "    print(\"Found issues:\")\n",
    "    for p in problems:\n",
    "        print(\"-\", p)\n",
    "else:\n",
    "    print(\"✅ All validations passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2269028-64fa-4016-b9f3-7bddaaa7b5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Drop rows where 0-1 columns are invalid\n",
    "columns_0_1 = [\"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\"]\n",
    "\n",
    "condition_0_1 = \" AND \".join([f\"({col_name} >= 0 AND {col_name} <= 1)\" for col_name in columns_0_1])\n",
    "\n",
    "# 2. Drop rows where loudness is invalid\n",
    "condition_loudness = \"(loudness >= -60 AND loudness <= 0)\"\n",
    "\n",
    "# 3. Drop rows where tempo is invalid\n",
    "condition_tempo = \"(tempo > 0 AND tempo <= 300)\"\n",
    "\n",
    "# 4. Drop rows where duration is invalid\n",
    "condition_duration = \"(duration_ms > 0 AND duration_minutes > 0)\"\n",
    "\n",
    "# 5. Combine all conditions\n",
    "full_condition = f\"{condition_0_1} AND {condition_loudness} AND {condition_tempo} AND {condition_duration}\"\n",
    "\n",
    "# 6. Apply filter\n",
    "dftransform = dftransform.filter(full_condition)\n",
    "\n",
    "print(\"✅ Invalid rows dropped successfully.\")\n",
    "print(f\"🧮 New number of rows: {dftransform.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8970d6f-c84f-4eee-8172-cfd75438b1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Add release_year column\n",
    "dftransform = dftransform.withColumn(\"album_release_year\", year(col(\"album_release_date\")))\n",
    "\n",
    "print(\"✅ Created new column 'album_release_year'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425588cb-f60f-4acf-9428-0e7084dbd975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform = dftransform.withColumnRenamed('name', 'track_name') \\\n",
    "    .withColumnRenamed('artists', 'artist_name') \\\n",
    "    .withColumnRenamed('loudness', 'loudness_db') \\\n",
    "    .withColumnRenamed('tempo', 'tempo_bpm')\n",
    "# (Rest are already clean and good)\n",
    "\n",
    "print(\"✅ Columns renamed for better readability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f351e9df-e216-4f47-92a5-9f1658bbb3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desired_order = [\n",
    "    'spotify_id', 'track_name', 'artist_name', 'daily_rank', 'country', 'snapshot_date',\n",
    "    'popularity', 'is_explicit', 'duration_ms', 'duration_minutes',\n",
    "    'album_name', 'album_release_date', 'album_release_year',\n",
    "    'danceability', 'energy', 'loudness_db', 'speechiness', 'acousticness',\n",
    "    'instrumentalness', 'liveness', 'valence', 'tempo_bpm'\n",
    "]\n",
    "dftransform = dftransform.select(*desired_order)\n",
    "\n",
    "print(\"✅ Columns rearranged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617a4ee8-add8-471d-bb79-00fba49dcc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftransform.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277e5de2-f8ea-4061-ac83-1931d561eaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/bronze/Silver/spotify_tracks\"\n",
    "dftransform.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n",
    "print(\"✅ Data successfully saved to Silver layer in Delta format!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46dda7d-adea-4f94-8ba9-2c69d354bdd4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_5258bb91\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_4286bc92\",\"enabled\":true,\"columnId\":\"snapshot_date\",\"dataType\":\"date\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1745742611870},{\"enabled\":true,\"filterGroupId\":\"fg_b9c5b4d7\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_119cda1\",\"enabled\":true,\"columnId\":\"snapshot_date\",\"dataType\":\"date\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1745742620121}],\"syncTimestamp\":1745742620121}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsilver=spark.read.format(\"delta\").load(\"/mnt/bronze/Silver/spotify_tracks\")\n",
    "dfsilver.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e02408d-e9eb-4bac-9e9c-ccdbc370ddf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsilver.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d895592-7ee2-4ee7-802d-cfb046ff03c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from pyspark.sql.functions import year, month, dayofmonth, monotonically_increasing_id, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assume dfsilver is your starting dataframe\n",
    "df = dfsilver\n",
    "\n",
    "# ============================\n",
    "# 1. Create Dimension Tables\n",
    "# ============================\n",
    "\n",
    "# 1.1 dim_track\n",
    "dim_track = df.select(\n",
    "    \"spotify_id\", \"track_name\", \"is_explicit\"\n",
    ").dropDuplicates().withColumn(\n",
    "    \"track_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# 1.2 dim_artist\n",
    "dim_artist = df.select(\n",
    "    \"artist_name\"\n",
    ").dropDuplicates().withColumn(\n",
    "    \"artist_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# 1.3 dim_album\n",
    "dim_album = df.select(\n",
    "    \"album_name\", \"album_release_date\", \"album_release_year\"\n",
    ").dropDuplicates().withColumn(\n",
    "    \"album_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# 1.4 dim_date\n",
    "dim_date = df.select(\n",
    "    \"snapshot_date\"\n",
    ").dropDuplicates().withColumn(\n",
    "    \"year\", year(\"snapshot_date\")\n",
    ").withColumn(\n",
    "    \"month\", month(\"snapshot_date\")\n",
    ").withColumn(\n",
    "    \"day\", dayofmonth(\"snapshot_date\")\n",
    ").withColumn(\n",
    "    \"date_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# 1.5 dim_country\n",
    "dim_country = df.select(\n",
    "    \"country\"\n",
    ").dropDuplicates().withColumn(\n",
    "    \"country_id\", monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 2. Create Fact Table\n",
    "# ============================\n",
    "\n",
    "# Now join df with dimension tables to get foreign keys\n",
    "\n",
    "# Join with dim_track\n",
    "fact = df.join(dim_track, on=[\"spotify_id\", \"track_name\", \"is_explicit\"], how=\"left\")\n",
    "\n",
    "# Join with dim_artist\n",
    "fact = fact.join(dim_artist, on=\"artist_name\", how=\"left\")\n",
    "\n",
    "# Join with dim_album\n",
    "fact = fact.join(dim_album, on=[\"album_name\", \"album_release_date\", \"album_release_year\"], how=\"left\")\n",
    "\n",
    "# Join with dim_date\n",
    "fact = fact.join(dim_date, on=\"snapshot_date\", how=\"left\")\n",
    "\n",
    "# Join with dim_country\n",
    "fact = fact.join(dim_country, on=\"country\", how=\"left\")\n",
    "\n",
    "# Select the final fact columns\n",
    "fact_track_rank = fact.select(\n",
    "    \"track_id\",\n",
    "    \"artist_id\",\n",
    "    \"album_id\",\n",
    "    \"date_id\",\n",
    "    \"country_id\",\n",
    "    \"daily_rank\",\n",
    "    \"popularity\",\n",
    "    \"duration_ms\",\n",
    "    \"duration_minutes\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"loudness_db\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo_bpm\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. Save Tables to Gold Layer\n",
    "# ============================\n",
    "\n",
    "# Set your Gold Layer path\n",
    "gold_path = \"/mnt/bronze/Gold/\"\n",
    "\n",
    "# Save dimension tables\n",
    "dim_track.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"dim_track\")\n",
    "dim_artist.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"dim_artist\")\n",
    "dim_album.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"dim_album\")\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"dim_date\")\n",
    "dim_country.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"dim_country\")\n",
    "\n",
    "# Save fact table\n",
    "fact_track_rank.write.format(\"delta\").mode(\"overwrite\").save(gold_path + \"fact_track_rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8f2809-6064-4b67-9fa5-3e8e0e889391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Dimension Tables\n",
    "dim_track = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/dim_track\")\n",
    "dim_artist = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/dim_artist\")\n",
    "dim_album = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/dim_album\")\n",
    "dim_date = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/dim_date\")\n",
    "dim_country = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/dim_country\")\n",
    "\n",
    "# Read Fact Table\n",
    "fact_track_rank = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/fact_track_rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b709e4-5eea-4f78-9287-49e70e47a780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_track_rank.createOrReplaceTempView(\"fact_track_rank\")\n",
    "dim_track.createOrReplaceTempView(\"dim_track\")\n",
    "dim_artist.createOrReplaceTempView(\"dim_artist\")\n",
    "dim_album.createOrReplaceTempView(\"dim_album\")\n",
    "dim_country.createOrReplaceTempView(\"dim_country\")\n",
    "dim_date.createOrReplaceTempView(\"dim_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0960ac4-89bb-4772-955a-8f5c07f385e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Top 10 songs globally based on avg popularity\n",
    "top_songs_global = fact_track_rank.groupBy(\"track_id\")\\\n",
    "    .agg(F.avg(\"popularity\").alias(\"avg_popularity\"))\\\n",
    "    .orderBy(\"avg_popularity\", ascending=False)\\\n",
    "    .limit(10)\n",
    "\n",
    "# Save to Gold\n",
    "top_songs_global.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/bronze/Gold/top_songs_global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b732ff4-80ba-46e6-8881-5abe0b722d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dG9wX3NvbmdzX2dsb2JhbF9kZiA9IHNwYXJrLnJlYWQuZm9ybWF0KCJkZWx0YSIpLmxvYWQoIi9tbnQvZ29sZC90b3Bfc29uZ3NfZ2xvYmFsIikKdG9wX3NvbmdzX2dsb2JhbF9kZi5jcmVhdGVPclJlcGxhY2VUZW1wVmlldygidG9wX3NvbmdzX2dsb2JhbCIpCnRvcF9zb25nc19nbG9iYWxfZGYuZGlzcGxheSgpCg==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2c8984f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2c8984f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2c8984f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2c8984f) SELECT `track_id`,SUM(`avg_popularity`) `column_1efc5896158` FROM q GROUP BY `track_id`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2c8984f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "track_id",
             "id": "column_1efc5896157"
            },
            "y": [
             {
              "column": "avg_popularity",
              "id": "column_1efc5896158",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_1efc5896158": {
             "name": "avg_popularity",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "f26b4370-0c4f-4d60-bc35-841440998e7a",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 56.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "track_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "track_id",
           "type": "column"
          },
          {
           "alias": "column_1efc5896158",
           "args": [
            {
             "column": "avg_popularity",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_songs_global_df = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/top_songs_global\")\n",
    "top_songs_global_df.createOrReplaceTempView(\"top_songs_global\")\n",
    "top_songs_global_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c1047c-e867-44f7-863c-dbba685b479c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 artists globally based on avg popularity\n",
    "top_artists_global = fact_track_rank.groupBy(\"artist_id\")\\\n",
    "    .agg(F.avg(\"popularity\").alias(\"avg_popularity\"))\\\n",
    "    .orderBy(\"avg_popularity\", ascending=False)\\\n",
    "    .limit(10)\n",
    "\n",
    "# Save to Gold\n",
    "top_artists_global.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/bronze/Gold/top_artists_global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770253b9-5bff-4d98-81b4-559cfc455e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dG9wX2FydGlzdHNfZ2xvYmFsX2RmID0gc3BhcmsucmVhZC5mb3JtYXQoImRlbHRhIikubG9hZCgiL21udC9nb2xkL3RvcF9hcnRpc3RzX2dsb2JhbCIpCnRvcF9hcnRpc3RzX2dsb2JhbF9kZi5jcmVhdGVPclJlcGxhY2VUZW1wVmlldygidG9wX2FydGlzdHNfZ2xvYmFsIikKdG9wX2FydGlzdHNfZ2xvYmFsX2RmLmRpc3BsYXkoKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView433ddb9\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView433ddb9\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView433ddb9\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView433ddb9) SELECT `artist_id`,SUM(`avg_popularity`) `column_1efc5896168`,`artist_id` FROM q GROUP BY `artist_id`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView433ddb9\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "artist_id",
             "id": "column_1efc5896173"
            },
            "x": {
             "column": "artist_id",
             "id": "column_1efc5896167"
            },
            "y": [
             {
              "column": "avg_popularity",
              "id": "column_1efc5896168",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "percentValues": false,
            "stacking": null
           },
           "seriesOptions": {
            "column_1efc5896168": {
             "name": "avg_popularity",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "2b725073-d945-4ef0-89bc-35066267961e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 58.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "artist_id",
           "type": "column"
          },
          {
           "column": "artist_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "artist_id",
           "type": "column"
          },
          {
           "alias": "column_1efc5896168",
           "args": [
            {
             "column": "avg_popularity",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "artist_id",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_artists_global_df = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/top_artists_global\")\n",
    "top_artists_global_df.createOrReplaceTempView(\"top_artists_global\")\n",
    "top_artists_global_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c717db6e-ed3c-4dd9-9e03-b95f2061edc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join fact_track_rank with dim_track to access 'is_explicit'\n",
    "track_data = fact_track_rank.join(dim_track, fact_track_rank.track_id == dim_track.track_id, \"inner\")\n",
    "\n",
    "# Group by 'is_explicit' and calculate the average popularity\n",
    "explicit_popularity = track_data.groupBy(\"is_explicit\")\\\n",
    "    .agg(F.avg(\"popularity\").alias(\"avg_popularity\"))\\\n",
    "    .orderBy(\"is_explicit\")\n",
    "\n",
    "# Save to Gold\n",
    "explicit_popularity.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/bronze/Gold/explicit_popularity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f5ae97-e81b-4446-9279-76e369b7b1b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZXhwbGljaXRfcG9wdWxhcml0eV9kZiA9IHNwYXJrLnJlYWQuZm9ybWF0KCJkZWx0YSIpLmxvYWQoIi9tbnQvZ29sZC9leHBsaWNpdF9wb3B1bGFyaXR5IikKZXhwbGljaXRfcG9wdWxhcml0eV9kZi5jcmVhdGVPclJlcGxhY2VUZW1wVmlldygiZXhwbGljaXRfcG9wdWxhcml0eSIpCmV4cGxpY2l0X3BvcHVsYXJpdHlfZGYuZGlzcGxheSgp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView929c373\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView929c373\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView929c373\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView929c373) SELECT `is_explicit`,SUM(`avg_popularity`) `column_1efc5896181` FROM q GROUP BY `is_explicit`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView929c373\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "is_explicit",
             "id": "column_1efc5896180"
            },
            "y": [
             {
              "column": "avg_popularity",
              "id": "column_1efc5896181",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_1efc5896181": {
             "name": "avg_popularity",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "d976273f-38a2-47d1-a45d-93e9e0788524",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 60.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "is_explicit",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "is_explicit",
           "type": "column"
          },
          {
           "alias": "column_1efc5896181",
           "args": [
            {
             "column": "avg_popularity",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explicit_popularity_df = spark.read.format(\"delta\").load(\"/mnt/bronze/Gold/explicit_popularity\")\n",
    "explicit_popularity_df.createOrReplaceTempView(\"explicit_popularity\")\n",
    "explicit_popularity_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd9d228-db52-45aa-b048-641ed3326560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7921761419652489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "projects-spotify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
